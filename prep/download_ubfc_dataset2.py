#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»Google Driveä¸‹è½½UBFC-rPPG Dataset 2çš„è„šæœ¬
ä½¿ç”¨æ–¹æ³•ï¼š
    python download_ubfc_dataset2.py
"""

import os
import sys
import subprocess
from pathlib import Path
import json

def check_gdown():
    """æ£€æŸ¥æ˜¯å¦å®‰è£…äº†gdown"""
    try:
        import gdown
        return True
    except ImportError:
        print("gdownæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
        return True

def check_downloaded_files(download_dir):
    """
    æ£€æŸ¥å·²ä¸‹è½½çš„æ–‡ä»¶ï¼Œè¿”å›ä¸‹è½½çŠ¶æ€
    
    Returns:
        dict: {
            'completed': [å·²å®Œæ•´ä¸‹è½½çš„subjectåˆ—è¡¨],
            'partial': [éƒ¨åˆ†ä¸‹è½½çš„subjectåˆ—è¡¨ï¼ˆæœ‰.partæ–‡ä»¶ï¼‰],
            'missing': [ç¼ºå¤±çš„subjectåˆ—è¡¨],
            'total_size': å·²ä¸‹è½½çš„æ€»å¤§å°ï¼ˆGBï¼‰
        }
    """
    download_dir = Path(download_dir)
    
    # UBFC Dataset 2åº”è¯¥æœ‰42ä¸ªsubjectsï¼ˆsubject1-subject49ï¼Œä½†æœ‰äº›ç¼ºå¤±ï¼‰
    # æ ¹æ®å®é™…ä¸‹è½½æƒ…å†µï¼Œæˆ‘ä»¬æ£€æŸ¥æ‰€æœ‰å­˜åœ¨çš„subjectæ–‡ä»¶å¤¹
    expected_subjects = set()
    completed_subjects = []
    partial_subjects = []
    
    total_size = 0
    
    # æ£€æŸ¥æ‰€æœ‰subjectæ–‡ä»¶å¤¹
    if download_dir.exists():
        for subject_dir in download_dir.iterdir():
            if subject_dir.is_dir() and subject_dir.name.startswith('subject'):
                subject_name = subject_dir.name
                expected_subjects.add(subject_name)
                
                # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
                vid_file = subject_dir / "vid.avi"
                gt_file = subject_dir / "ground_truth.txt"
                
                # æ£€æŸ¥æ˜¯å¦æœ‰.partæ–‡ä»¶ï¼ˆæœªå®Œæˆçš„ä¸‹è½½ï¼‰
                part_files = list(subject_dir.glob("*.part"))
                
                if part_files:
                    # æœ‰.partæ–‡ä»¶ï¼Œè¯´æ˜ä¸‹è½½ä¸­æ–­
                    partial_subjects.append(subject_name)
                    print(f"  âš ï¸  {subject_name}: éƒ¨åˆ†ä¸‹è½½ï¼ˆæœ‰.partæ–‡ä»¶ï¼‰")
                    # è®¡ç®—å·²ä¸‹è½½å¤§å°
                    for part_file in part_files:
                        total_size += part_file.stat().st_size / (1024**3)
                elif vid_file.exists() and gt_file.exists():
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆç†ï¼ˆvid.aviåº”è¯¥>100MBï¼‰
                    vid_size = vid_file.stat().st_size / (1024**3)
                    if vid_size > 0.1:  # è‡³å°‘100MB
                        completed_subjects.append(subject_name)
                        total_size += vid_size
                        total_size += gt_file.stat().st_size / (1024**3)
                    else:
                        partial_subjects.append(subject_name)
                        print(f"  âš ï¸  {subject_name}: æ–‡ä»¶ä¸å®Œæ•´ï¼ˆvid.aviå¤ªå°: {vid_size:.2f}GBï¼‰")
                else:
                    partial_subjects.append(subject_name)
                    print(f"  âš ï¸  {subject_name}: æ–‡ä»¶ç¼ºå¤±")
    
    # æ ¹æ®å®é™…å‘ç°çš„subjectsï¼Œæ¨æ–­åº”è¯¥æœ‰å“ªäº›subjects
    # UBFC Dataset 2é€šå¸¸æœ‰: subject1, subject3, subject4, subject5, subject8-subject49ï¼ˆéƒ¨åˆ†ç¼ºå¤±ï¼‰
    # ä½†æˆ‘ä»¬åªæ£€æŸ¥å®é™…å­˜åœ¨çš„æ–‡ä»¶å¤¹
    
    return {
        'completed': completed_subjects,
        'partial': partial_subjects,
        'total_size': total_size,
        'found_subjects': list(expected_subjects)
    }

def clean_partial_files(download_dir):
    """æ¸…ç†æœªå®Œæˆçš„.partæ–‡ä»¶"""
    download_dir = Path(download_dir)
    cleaned = []
    
    if download_dir.exists():
        for part_file in download_dir.rglob("*.part"):
            # æ£€æŸ¥å¯¹åº”çš„å®Œæ•´æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            # .partæ–‡ä»¶åæ ¼å¼å¯èƒ½æ˜¯: filename.part æˆ– filename.extxxxxx.part
            part_name = part_file.name
            
            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„å®Œæ•´æ–‡ä»¶
            # å¦‚æœæ˜¯ vid.avi7idub7yy.partï¼Œå¯¹åº”çš„æ–‡ä»¶åº”è¯¥æ˜¯ vid.avi
            parent_dir = part_file.parent
            possible_names = []
            
            # å¦‚æœ.partæ–‡ä»¶ååŒ…å«åŸå§‹æ–‡ä»¶åï¼Œæå–å®ƒ
            if 'vid.avi' in part_name:
                possible_names.append('vid.avi')
            if 'ground_truth.txt' in part_name:
                possible_names.append('ground_truth.txt')
            
            # ä¹Ÿæ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶åï¼ˆå»æ‰.partå’Œéšæœºåç¼€ï¼‰
            base_name = part_name.replace('.part', '')
            # å°è¯•å»æ‰å¯èƒ½çš„éšæœºåç¼€ï¼ˆå¦‚ vid.avi7idub7yy -> vid.aviï¼‰
            if '.' in base_name:
                parts = base_name.split('.')
                if len(parts) >= 2:
                    # å‡è®¾æœ€åä¸€éƒ¨åˆ†æ˜¯æ‰©å±•åï¼Œå‰é¢å¯èƒ½æœ‰éšæœºåç¼€
                    ext = parts[-1]
                    # å°è¯•æ‰¾åˆ°åŸå§‹æ–‡ä»¶å
                    for f in parent_dir.glob(f"*.{ext}"):
                        if f.name != part_name and not f.name.endswith('.part'):
                            possible_names.append(f.name)
            
            # å¦‚æœæ‰¾åˆ°å¯¹åº”çš„å®Œæ•´æ–‡ä»¶ï¼Œåˆ é™¤.partæ–‡ä»¶
            should_delete = False
            if possible_names:
                for name in possible_names:
                    full_file = parent_dir / name
                    if full_file.exists() and full_file.stat().st_size > 1000:  # è‡³å°‘1KB
                        should_delete = True
                        print(f"  æ¸…ç†.partæ–‡ä»¶ï¼ˆå®Œæ•´æ–‡ä»¶å·²å­˜åœ¨ï¼‰: {part_file.name} -> {name}")
                        break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”æ–‡ä»¶ï¼Œä¹Ÿåˆ é™¤.partæ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯æ—§çš„ä¸‹è½½æ®‹ç•™ï¼‰
                should_delete = True
                print(f"  æ¸…ç†æ®‹ç•™.partæ–‡ä»¶: {part_file.name}")
            
            if should_delete:
                part_file.unlink()
                cleaned.append(str(part_file))
    
    return cleaned

def download_dataset2(workers=4, resume=True):
    """
    ä¸‹è½½UBFC-rPPG Dataset 2ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    
    Args:
        workers: å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼Œå¯ä»¥å¢åŠ åˆ°8-16ä»¥åŠ é€Ÿï¼‰
        resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤Trueï¼‰
    """
    # Google Driveæ–‡ä»¶å¤¹ID - DATASET_2çš„ç›´æ¥ID
    folder_id = "1q4vWuF2GJvKP5xyeX8dxaJ2fmq97-4ai"
    
    # ä¸‹è½½ç›®å½• - ç›´æ¥ä¸‹è½½åˆ°DATASET_2æ–‡ä»¶å¤¹
    download_dir = Path(__file__).parent.parent / "datasets" / "UBFC_raw" / "DATASET_2"
    download_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nä¸‹è½½ç›®å½•: {download_dir}")
    
    # æ£€æŸ¥å·²ä¸‹è½½çš„æ–‡ä»¶
    if resume:
        print("\n" + "="*60)
        print("æ£€æŸ¥å·²ä¸‹è½½çš„æ–‡ä»¶...")
        print("="*60)
        status = check_downloaded_files(download_dir)
        
        print(f"\nğŸ“Š ä¸‹è½½çŠ¶æ€:")
        print(f"  âœ… å·²å®Œæˆ: {len(status['completed'])} ä¸ªsubjects")
        if status['completed']:
            print(f"     åˆ—è¡¨: {', '.join(sorted(status['completed']))}")
        
        print(f"  âš ï¸  éƒ¨åˆ†ä¸‹è½½: {len(status['partial'])} ä¸ªsubjects")
        if status['partial']:
            print(f"     åˆ—è¡¨: {', '.join(sorted(status['partial']))}")
        
        print(f"  ğŸ“¦ å·²ä¸‹è½½æ€»å¤§å°: {status['total_size']:.2f} GB")
        
        if status['partial']:
            print(f"\nâš ï¸  å‘ç° {len(status['partial'])} ä¸ªæœªå®Œæˆçš„ä¸‹è½½")
            print("  å°†æ¸…ç†.partæ–‡ä»¶å¹¶é‡æ–°ä¸‹è½½è¿™äº›subjects...")
            cleaned = clean_partial_files(download_dir)
            if cleaned:
                print(f"  å·²æ¸…ç† {len(cleaned)} ä¸ª.partæ–‡ä»¶")
        
        if len(status['completed']) > 0:
            print(f"\nâœ… å°†è·³è¿‡å·²å®Œæˆçš„ {len(status['completed'])} ä¸ªsubjects")
            print("  gdownä¼šè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œåªä¸‹è½½ç¼ºå¤±çš„æ–‡ä»¶")
    
    print(f"\nå¼€å§‹ä¸‹è½½UBFC-rPPG Dataset 2...")
    print(f"âš ï¸  æ³¨æ„: åªä¸‹è½½DATASET_2ï¼Œä¸åŒ…æ‹¬DATASET_1")
    print(f"ğŸ“¥ å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°: {workers} (å¯ä»¥é€šè¿‡--workerså‚æ•°è°ƒæ•´)")
    print(f"ğŸ”„ æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
    
    import gdown
    
    # é‡è¦ï¼šgdownçš„download_folderå¯èƒ½ä¸ä¼šè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
    # æˆ‘ä»¬éœ€è¦åœ¨ä¸‹è½½å‰æ‰‹åŠ¨æ£€æŸ¥å¹¶ä¸´æ—¶é‡å‘½åå·²å®Œæˆçš„æ–‡ä»¶
    # è¿™æ ·gdownä¼šè·³è¿‡å®ƒä»¬ï¼Œä¸‹è½½å®Œæˆåå†æ¢å¤
    
    if resume:
        print(f"\nå‡†å¤‡è·³è¿‡å·²å®Œæˆçš„æ–‡ä»¶...")
        completed_files = []
        for subject_dir in download_dir.iterdir():
            if subject_dir.is_dir() and subject_dir.name.startswith('subject'):
                vid_file = subject_dir / "vid.avi"
                gt_file = subject_dir / "ground_truth.txt"
                
                # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼ˆè‡³å°‘100MBï¼‰
                if vid_file.exists() and vid_file.stat().st_size > 100 * 1024 * 1024:
                    # ä¸´æ—¶é‡å‘½åï¼Œè®©gdownè®¤ä¸ºæ–‡ä»¶ä¸å­˜åœ¨
                    temp_name = vid_file.with_suffix('.avi.tmp_skip')
                    vid_file.rename(temp_name)
                    completed_files.append(('vid', temp_name, vid_file))
                    print(f"  è·³è¿‡: {subject_dir.name}/vid.avi")
                
                # æ£€æŸ¥ground_truthæ–‡ä»¶
                if gt_file.exists() and gt_file.stat().st_size > 1000:
                    temp_name = gt_file.with_suffix('.txt.tmp_skip')
                    gt_file.rename(temp_name)
                    completed_files.append(('gt', temp_name, gt_file))
        
        print(f"  å·²æ ‡è®° {len([f for f in completed_files if f[0]=='vid'])} ä¸ªè§†é¢‘æ–‡ä»¶è·³è¿‡")
    
    # ä¸‹è½½DATASET_2æ–‡ä»¶å¤¹
    url = f"https://drive.google.com/drive/folders/{folder_id}"
    
    # gdownçš„download_folderé»˜è®¤ä¼šè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ 
    try:
        gdown.download_folder(
            url, 
            output=str(download_dir), 
            quiet=False, 
            use_cookies=False,
            remaining_ok=True  # å…è®¸æ–­ç‚¹ç»­ä¼ ï¼Œè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        )
    except Exception as e:
        print(f"âš ï¸  ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("å°è¯•ä½¿ç”¨åŸºæœ¬ä¸‹è½½æ–¹å¼...")
        gdown.download_folder(
            url, 
            output=str(download_dir), 
            quiet=False, 
            use_cookies=False
        )
    finally:
        # æ¢å¤ä¸´æ—¶é‡å‘½åçš„æ–‡ä»¶
        if resume and 'completed_files' in locals():
            print(f"\næ¢å¤å·²å®Œæˆçš„æ–‡ä»¶...")
            for file_type, temp_path, original_path in completed_files:
                if temp_path.exists():
                    temp_path.rename(original_path)
                    print(f"  æ¢å¤: {original_path.parent.name}/{original_path.name}")
    
    # å†æ¬¡æ£€æŸ¥ä¸‹è½½çŠ¶æ€
    print("\n" + "="*60)
    print("ä¸‹è½½å®Œæˆï¼Œæ£€æŸ¥æœ€ç»ˆçŠ¶æ€...")
    print("="*60)
    final_status = check_downloaded_files(download_dir)
    print(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
    print(f"  âœ… å·²å®Œæˆ: {len(final_status['completed'])} ä¸ªsubjects")
    print(f"  âš ï¸  éƒ¨åˆ†ä¸‹è½½: {len(final_status['partial'])} ä¸ªsubjects")
    print(f"  ğŸ“¦ æ€»å¤§å°: {final_status['total_size']:.2f} GB")
    
    if final_status['partial']:
        print(f"\nâš ï¸  ä»æœ‰ {len(final_status['partial'])} ä¸ªsubjectsæœªå®Œæˆä¸‹è½½")
        print("  å¯ä»¥é‡æ–°è¿è¡Œæ­¤è„šæœ¬ç»§ç»­ä¸‹è½½")
    
    print(f"\næ–‡ä»¶ä¿å­˜åœ¨: {download_dir}")
    return download_dir

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¸‹è½½UBFC-rPPG Dataset 2ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰')
    parser.add_argument('--workers', type=int, default=4,
                        help='å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼Œå¯ä»¥å¢åŠ åˆ°8-16ä»¥åŠ é€Ÿï¼Œä½†å—ç½‘ç»œå¸¦å®½é™åˆ¶ï¼‰')
    parser.add_argument('--no-resume', action='store_true',
                        help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°ä¸‹è½½æ‰€æœ‰æ–‡ä»¶')
    args = parser.parse_args()
    
    print("="*60)
    print("UBFC-rPPG Dataset 2 ä¸‹è½½è„šæœ¬")
    print("="*60)
    
    print("\nğŸ“Œ å…³äºä¸‹è½½é€Ÿåº¦çš„è¯´æ˜:")
    print("  - GPUä¸èƒ½ç”¨äºä¸‹è½½åŠ é€Ÿï¼ˆGPUåªç”¨äºè®¡ç®—ï¼Œä¸ç”¨äºç½‘ç»œä¼ è¾“ï¼‰")
    print("  - ä¸‹è½½é€Ÿåº¦ä¸»è¦å—é™äº:")
    print("    1. ç½‘ç»œå¸¦å®½ï¼ˆä½ çš„UbuntuæœåŠ¡å™¨åˆ°Google Driveçš„è¿æ¥é€Ÿåº¦ï¼‰")
    print("    2. Google Driveçš„ä¸‹è½½é€Ÿåº¦é™åˆ¶")
    print("    3. ç£ç›˜I/Oé€Ÿåº¦")
    print("  - å¯ä»¥é€šè¿‡å¢åŠ å¹¶å‘çº¿ç¨‹æ•°æ¥åŠ é€Ÿï¼ˆä½†ä¸è¦è¶…è¿‡ç½‘ç»œå¸¦å®½ï¼‰")
    print("  - å»ºè®®: å¦‚æœç½‘ç»œå¸¦å®½å……è¶³ï¼Œå¯ä»¥è®¾ç½® --workers 8 æˆ– 16")
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    import shutil
    total, used, free = shutil.disk_usage("/home/vt_ai_test1")
    print(f"\nç£ç›˜ç©ºé—´æ£€æŸ¥:")
    print(f"  æ€»ç©ºé—´: {total // (1024**3)} GB")
    print(f"  å·²ä½¿ç”¨: {used // (1024**3)} GB")
    print(f"  å¯ç”¨ç©ºé—´: {free // (1024**3)} GB")
    
    if free < 50 * (1024**3):  # è‡³å°‘éœ€è¦50GB
        print("\nâš ï¸  è­¦å‘Š: å¯ç”¨ç©ºé—´å¯èƒ½ä¸è¶³ï¼Œå»ºè®®è‡³å°‘ä¿ç•™50GBç©ºé—´")
        response = input("æ˜¯å¦ç»§ç»­ä¸‹è½½? (y/n): ")
        if response.lower() != 'y':
            sys.exit(0)
    
    # æ£€æŸ¥å¹¶å®‰è£…gdown
    check_gdown()
    
    # ä¸‹è½½æ•°æ®é›†
    try:
        download_dir = download_dataset2(workers=args.workers, resume=not args.no_resume)
        print(f"\nâœ… ä¸‹è½½å®Œæˆï¼")
        print(f"è¯·æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶ï¼Œç„¶åè¿è¡Œé¢„å¤„ç†è„šæœ¬ç”Ÿæˆh5æ–‡ä»¶")
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nå¦‚æœgdownä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æ‰‹åŠ¨ä¸‹è½½:")
        print("1. è®¿é—®: https://drive.google.com/drive/folders/1o0XU4gTIo46YfwaWjIgbtCncc-oF44Xk")
        print("2. ä¸‹è½½DATASET_2æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰subjects")
        print("3. å°†æ–‡ä»¶è§£å‹åˆ° datasets/UBFC_raw/ ç›®å½•")
        sys.exit(1)
